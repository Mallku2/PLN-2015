 * Ejercicio 1: Evaluación de Parsers
    Sobre lo ya implementado en eval.py, se agregó lo solicitado.
    La evaluación de los modelos "baseline" arrojó los siguientes resultados:

    _ FLAT:
        Parsed 1444 sentences

        Labeled
          Precision: 99.93%
          Recall: 14.57%
          F1: 25.43%

        Unlabeled
          Precision: 100.00%
          Recall: 14.58%
          F1: 25.45%

        real	0m5.397s
        user	0m5.336s
        sys	0m0.392s


    _ RBRANCH:
        Parsed 1444 sentences

        Labeled
          Precision: 8.81%
          Recall: 14.57%
          F1: 10.98%

        Unlabeled
          Precision: 8.87%
          Recall: 14.68%
          F1: 11.06%

        real	0m5.980s
        user	0m5.912s
        sys	0m0.376s


    _ LBRANCH:
        Parsed 1444 sentences

        Labeled
          Precision: 8.81%
          Recall: 14.57%
          F1: 10.98%

        Unlabeled
          Precision: 14.71%
          Recall: 24.33%
          F1: 18.33%

        real	0m5.901s
        user	0m5.812s
        sys	0m0.360s

  Notar cómo el modelo FLAT tiene alto nivel de presición: de hecho,
prácticamente todos los constituyentes que devuelve, son correctos. Sin
embargo, no devuelve todos los que debía, y allí es en donde el
estadístico "recall" agrega información que permite entender mejor el
rendimiento del modelo.


 * Ejercicio 2: Algoritmo CKY
    Se implementó el algoritmo CKY, siguiendo las sugerencias de las filminas
de Collins, junto con la utilización de un diccionario que hace de "cache" de
producciones, para poder obtener la información de las mismas de manera más
ágil.
    Se agregó, a la suite de tests, un caso de una oración con más de un
análisis posible. La oración se extrajo de la videolecture "Examples of
Ambiguity", del ejemplo sobre "POS ambiguity", referido a la palabra
inglesa "duck", la cual puede ocurrir como verbo o sustantivo. De allí que
pueda existir más de un análisis, en una oración que la incluya. La oración
bajo análisis es 'the girl saw her duck with the telescope', y para analizarla
se definió la siguiente PCFG, en donde las probabilidades se asignaron de
manera arbitraria:

                S -> NP VP              [1.0]

                VP -> VP PP             [0.25]
                VP -> Vt NP             [0.25]
                VP -> V S               [0.25]
                VP -> Vi                [0.25]

                NP -> PRP NN            [0.5]
                NP -> Det Noun          [0.25]
                NP -> 'her'             [0.25]

                PP -> IN NP             [1.0]

                Vi -> 'duck'            [1.0]

                IN -> 'with'            [1.0]

                Det -> 'the'            [1.0]

                Noun -> 'girl'          [0.25]
                Noun -> 'telescope'     [0.75]

                Vt -> 'saw'             [1.0]

                PRP -> 'her'            [1.0]

                NN -> 'duck'            [1.0]

 * Ejercicio 3: PCFGs No Lexicalizadas
    Se implementó una PCFG no lexicalizada. El código resultante es sucinto,
gracias a los métodos que provee la clase Tree y procedimientos del módulo
nltk.grammar (que nos permiten obtener una PCFG en base a las producciones
ordinarios extraidas del corpus).
    Se entrenó el modelo resultante, y se lo evaluó sobre la misma porción
del corpus utilizada en el ejercicio 2. Obtuvimos los siguientes resultados:

          Parsed 1444 sentences
            Labeled
              Precision: 73.15%
              Recall: 72.85%
              F1: 73.00%

            Unlabeled
              Precision: 75.26%
              Recall: 74.95%
              F1: 75.10%

          real	1m24.555s
          user	1m23.944s
          sys	0m0.452s



 * Ejercicio 4: Markovización Horizontal
    Se modificó la UPCFG, para agregar la posibilidad de utilizar
markovización horizontal (MH). La implementación simplemente se apoya en el
método chomsky_normal_form de la clase Tree, el cual, opcionalmente, puede
realizar este trabajo.
    Como se señala en https://www.youtube.com/watch?v=a9qw0IFjojA, a medida
que, en una gramática binarizada, condicionamos sobre menor cantidad de
información, relativa al contexto  en el que una producción ocurre, reducimos
así la cantidad de símbolos de nuestra gramática, y para ciertos valores de MH,
hasta podemos lograr mejor rendimiento, a comparación de una gramática
binarizada sin restricción en la información contextual que mantenemos
codificada en los no-terminales. Esto se va a observar en los resultados
que obtuvimos.
    Se evaluaron modelos con MH de orden n variable, entre 0 y 3, sobre la
misma porción de corpus empleada para evaluar los modelos anteriores.
Obtuvimos los siguientes resultados:

    _ MH = 0:
      Parsed 1444 sentences
        Labeled
          Precision: 70.23%
          Recall: 70.00%
          F1: 70.12%

        Unlabeled
          Precision: 72.10%
          Recall: 71.87%
          F1: 71.99%

        real	0m52.663s
        user	0m51.840s
        sys	0m0.284s


    _ MH = 1:
      Parsed 1444 sentences
        Labeled
          Precision: 74.66%
          Recall: 74.57%
          F1: 74.61%
        Unlabeled
          Precision: 76.53%
          Recall: 76.43%
          F1: 76.48%

        real	0m57.065s
        user	0m56.160s
        sys	0m0.200s


    _ MH = 2:
      Parsed 1444 sentences
        Labeled
          Precision: 74.81%
          Recall: 74.29%
          F1: 74.55%
        Unlabeled
          Precision: 76.73%
          Recall: 76.20%
          F1: 76.46%

        real	1m8.547s
        user	1m8.476s
        sys	0m0.392s


    _ MH = 3:
      Parsed 1444 sentences
        Labeled
          Precision: 73.99%
          Recall: 73.35%
          F1: 73.67%
        Unlabeled
          Precision: 76.15%
          Recall: 75.49%
          F1: 75.82%

        real	1m20.238s
        user	1m20.052s
        sys	0m0.396s


 * Ejercicio 5 (punto bonus): CKY con Producciones Unarias
  Se modificó el algoritmo CKY para que pueda manejar producciones unarias,
adaptando el algoritmo expuesto por Jurafsky y Manning
en https://www.youtube.com/watch?v=hq80J8kBg-Y a nuestra versión,
que está tomada de las notas de Collins
(http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf). La
principal diferencie entre ambos algoritmos es que el primer intenta manejar
producciones "vacías", utilizando la técnica de "fenceposts", mientras que
nuestra implementación no.
  Se hicieron las correspondientes modificaciones a la implementación de la
UPCFG, para admitir producciones unarias, como también se modificó
el script train.py.
  Se agregó a la suite de tests un caso de test para gramáticas con
producciones unarias, tomado de la exposición de Jurafsky y Manning.
