* Ejercicio 1: Corpus Ancora: Estadísticas de etiquetas POS:
    Se recorrieron las oraciones etiquetadas del corpus "Ancora", extrayendo 
los estadísticos de interés. No fue necesario emplear ninguna librería externa,
ya que los datos calculados se redujeron a simples conteos, apoyandonós en 
diccionarios Python.
    Los datos obtenidos son los siguientes:
    
Estadísticas básicas:
    Cantidad de oraciones: 17379
    Cantidad de ocurrencias de palabras: 517268
    Cantidad de palabras: 46482
    Cantidad de etiquetas: 48

Etiquetas más frecuentes:

Etiqueta|Frecuencia|Porcentaje del total|Palabras más frecuentes
da      |     54552|              0.1055|'la': 17897, 'el': 14524, 'los': 7758, 'las': 4882, 'El': 2817
fp      |     21157|              0.0409|'.': 17513, '(': 1823, ')': 1821
rg      |     15333|             0.02964|'más': 1707, 'hoy': 772, 'también': 683, 'ayer': 593, 'ya': 544
nc      |     92002|              0.1779|'años': 849, 'presidente': 682, 'millones': 616, 'equipo': 457, 'partido': 438
aq      |     33904|             0.06554|'pasado': 393, 'gran': 275, 'mayor': 248, 'nuevo': 234, 'próximo': 213
vm      |     50609|             0.09784|'está': 564, 'tiene': 511, 'dijo': 499, 'puede': 381, 'hace': 350
cc      |     15023|             0.02904|'y': 11211, 'pero': 938, 'o': 895, 'Pero': 323, 'e': 310
np      |     29113|             0.05628|'Gobierno': 554, 'España': 380, 'PP': 234, 'Barcelona': 232, 'Madrid': 196
sp      |     79904|              0.1545|'de': 28475, 'en': 12114, 'a': 8192, 'del': 6518, 'con': 4150
fc      |     30148|             0.05828|',': 30148

Describimos a continuación el significado de cada etiqueta:
    _ 'da': categoría: determinante, tipo: artículo.
    _ 'fp': categoría: puntuación.
    _ 'rg': categoría: adverbio, tipo: general.
    _ 'nc': categoría: nombre, tipo: común.
    _ 'aq': categoría: adjetivo, tipo: calificativo.
    _ 'vm': categoría: verbo, tipo: principal.
    _ 'cc': categoría: conjunción, tipo: coordinada.
    _ 'np': categoría: nombre, tipo: propio.
    _ 'sp': categoría: adposición, tipo: preposición.
    _ 'fc': categoría: puntuación.

Niveles de ambigüedad de las palabras:

Nivel de ambigüedad |Cantidad de palabras|Porcentaje del total|Palabras más frecuentes
                   1|               44109|             0.08527|',': 30148, 'el': 14524, 'en': 12114, 'con': 4150, 'por': 4087
                   2|                2194|            0.004242|'la': 18100, 'y': 11212, '"': 9296, 'los': 7824, 'del': 6519
                   3|                 153|           0.0002958|'.': 17520, 'a': 8200, 'un': 5198, 'no': 3300, 'es': 2315
                   4|                  19|           3.673e-05|'de': 28478, 'dos': 917, 'este': 830, 'tres': 425, 'todo': 393
                   5|                   4|           7.733e-06|'que': 15391, 'mismo': 247, 'cinco': 224, 'medio': 105
                   6|                   3|             5.8e-06|'como': 1736, 'una': 3852, 'uno': 335


* Ejercicio 2: Baseline Tagger
    Se programó un etiquetador "baseline", en el módulo baseline.py. 
La implementación no supuso ningún desafío en particular.

* Ejercicio 3: Entrenamiento y Evaluación de Taggers
    Se programaron los scrips de entrenamiento y evaluación. El cálculo
de los estadísticos de interés se realizó sin ayuda de alguna librería externa,
salvo para el caso de la matriz de confusión, en donde se utilizó el procedimiento
confusion_matrix, del módulo metrics, de la librería sklearn.
    Se entrenó el etiquetador "baseline", programado en el ejercicio anterior.
La evaluación del mismo arrojó los siguientes resultados:
Accuracy: 89.01%
Accuracy over known words: 95.32%
Accuracy over unknown words: 31.80%

En la matriz de confusión se observan presencia de valores altos en la diagonal
de la misma, con algunos valores menores por fuera de la diagonal, lo que 
sugiere cierto grado de precisión, cuantificado por el valor de "accuracy" 
mostrado.

* Ejercicio 4: Hidden Markov Models y Algoritmo de Viterbi
    Se implementaron un modelo de Hidden Markov (HMM) y un algoritmo de 
Viterbi. La implementación del HMM resultó bastante directa, lo cual no fue
el caso con el algoritmo de Viterbi. Se intentó primero una transcripción 
directa del algoritmo, desde las notas de Collins, pero generalizarlo sobre el
orden n del modelo resultó, extrañamente, problemático. La implementación final
evita recorrer todas las combinaciones posibles de etiquetas, para concentrarse
en extender sólo secuencias de etiquetas para las cuales ya se sabe que las
probabilidades de transición y de "output" involucradas son distintas de 0.   

* Ejercicio 5: HMM POS Tagger
    Se implementó un HMM Pos Tagger, estimando sus parámetros mediante
el método de máxima verosimilitud. El modelo resultante se entrenó y evaluó,
para distintos valores de n. También se midió el tiempo de ejecución del
algoritmo de evaluación, con el comando time. Se obtuvieron los siguientes resultados:
    _ n = 1:
        Accuracy: 84.94%
        Accuracy over known words: 94.27%
        Accuracy over unknown words: 0.41%
        
        real    0m47.498s
        user    0m40.332s
        sys     0m0.668s
        
    _ n = 2:
        Accuracy: 92.21%
        Accuracy over known words: 97.33%
        Accuracy over unknown words: 45.75%
        
        real    0m47.525s
        user    0m43.120s
        sys     0m0.768s
   
   _ n = 3:
        Accuracy: 92.31%
        Accuracy over known words: 96.92%
        Accuracy over unknown words: 50.58%
                 
        real    3m2.780s
        user    2m46.952s
        sys 0m1.696s
        
    
* Ejercicio 6: Features para Etiquetado de Secuencias
    Se implementaron las features solicitadas. La labor no presentó mayores
desafíos. La descripción de las mismas resultó directa. 
    
* Ejercicio 7: Maximum Entropy Markov Models
    Se implementó un Maximum Entropy Markov Model (MEMM), empleando la librería
featureforge (para la construcción de la matriz de valores de los 
"features" sobre los historiales) y sklearn (de donde se extrajo el clasificador
LogisticRegression, y la clase Pipeline, para facilitar el proceso de
vectorización de los historiales junto con los "features", 
y entrenamiento y posterior uso del clasificador).
    El MEMM se entrenó y evaluó, para distintos valores de n. Nuevamente se
tomó el tiempo de evaluación, con el comando time. Se obtuvieron los siguientes
resultados:

    _ n = 1:
        Accuracy: 92.03%
        Accuracy over known words: 95.26%
        Accuracy over unknown words: 62.75%
        
        real    1m35.271s
        user    1m32.424s
        sys     0m0.736s
        
    _n = 2:
        Accuracy: 91.32%
        Accuracy over known words: 94.55%
        Accuracy over unknown words: 62.07%
        
        real    1m46.164s
        user    1m41.952s
        sys     0m0.616s
        
        


 

