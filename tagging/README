* Ejercicio 1: Corpus Ancora: Estadísticas de etiquetas POS:
    Se recorrieron las oraciones etiquetadas del corpus "Ancora", extrayendo 
los estadísticos de interés. No fue necesario emplear ninguna librería externa,
ya que los datos calculados se redujeron a simples conteos, apoyandonós en 
diccionarios Python.
    Los datos obtenidos son los siguientes:
    
Estadísticas básicas:
    Cantidad de oraciones: 17379
    Cantidad de ocurrencias de palabras: 517268
    Cantidad de palabras: 46482
    Cantidad de etiquetas: 48

Etiquetas más frecuentes:

Etiqueta|Frecuencia|Porcentaje del total|Palabras más frecuentes
da      |     54552|              0.1055|'la': 17897, 'el': 14524, 'los': 7758, 'las': 4882, 'El': 2817
fp      |     21157|              0.0409|'.': 17513, '(': 1823, ')': 1821
rg      |     15333|             0.02964|'más': 1707, 'hoy': 772, 'también': 683, 'ayer': 593, 'ya': 544
nc      |     92002|              0.1779|'años': 849, 'presidente': 682, 'millones': 616, 'equipo': 457, 'partido': 438
aq      |     33904|             0.06554|'pasado': 393, 'gran': 275, 'mayor': 248, 'nuevo': 234, 'próximo': 213
vm      |     50609|             0.09784|'está': 564, 'tiene': 511, 'dijo': 499, 'puede': 381, 'hace': 350
cc      |     15023|             0.02904|'y': 11211, 'pero': 938, 'o': 895, 'Pero': 323, 'e': 310
np      |     29113|             0.05628|'Gobierno': 554, 'España': 380, 'PP': 234, 'Barcelona': 232, 'Madrid': 196
sp      |     79904|              0.1545|'de': 28475, 'en': 12114, 'a': 8192, 'del': 6518, 'con': 4150
fc      |     30148|             0.05828|',': 30148

Describimos a continuación el significado de cada etiqueta:
    _ 'da': categoría: determinante, tipo: artículo.
    _ 'fp': categoría: puntuación.
    _ 'rg': categoría: adverbio, tipo: general.
    _ 'nc': categoría: nombre, tipo: común.
    _ 'aq': categoría: adjetivo, tipo: calificativo.
    _ 'vm': categoría: verbo, tipo: principal.
    _ 'cc': categoría: conjunción, tipo: coordinada.
    _ 'np': categoría: nombre, tipo: propio.
    _ 'sp': categoría: adposición, tipo: preposición.
    _ 'fc': categoría: puntuación.

Niveles de ambigüedad de las palabras:

Nivel de ambigüedad |Cantidad de palabras|Porcentaje del total|Palabras más frecuentes
                   1|               44109|             0.08527|',': 30148, 'el': 14524, 'en': 12114, 'con': 4150, 'por': 4087
                   2|                2194|            0.004242|'la': 18100, 'y': 11212, '"': 9296, 'los': 7824, 'del': 6519
                   3|                 153|           0.0002958|'.': 17520, 'a': 8200, 'un': 5198, 'no': 3300, 'es': 2315
                   4|                  19|           3.673e-05|'de': 28478, 'dos': 917, 'este': 830, 'tres': 425, 'todo': 393
                   5|                   4|           7.733e-06|'que': 15391, 'mismo': 247, 'cinco': 224, 'medio': 105
                   6|                   3|             5.8e-06|'como': 1736, 'una': 3852, 'uno': 335


* Ejercicio 2: Baseline Tagger
    Se programó un etiquetador "baseline", en el módulo baseline.py. 
La implementación no supuso ningún desafío en particular.

* Ejercicio 3: Entrenamiento y Evaluación de Taggers
    Se programaron los scrips de entrenamiento y evaluación. El cálculo
de los estadísticos de interés se realizó sin ayuda de alguna librería externa,
salvo para el caso de la matriz de confusión, en donde se utilizó el procedimiento
confusion_matrix, del módulo metrics, de la librería sklearn.
    Se entrenó el etiquetador "baseline", programado en el ejercicio anterior.
La evaluación del mismo arrojó los siguientes resultados:
Accuracy: 89.01%
Accuracy over known words: 95.32%
Accuracy over unknown words: 31.80%

En la matriz de confusión se observan presencia de valores altos en la diagonal
de la misma, con algunos valores menores por fuera de la diagonal, lo que 
sugiere cierto grado de precisión, cuantificado por el valor de "accuracy" 
mostrado.

* Ejercicio 4: Hidden Markov Models y Algoritmo de Viterbi
    Se implementaron un modelo de Hidden Markov (HMM) y un algoritmo de 
Viterbi. La implementación del HMM resultó bastante directa, lo cual no fue
el caso con el algoritmo de Viterbi. Se intentó primero una transcripción 
directa del algoritmo, desde las notas de Collins, pero generalizarlo sobre el
orden n del modelo resultó, extrañamente, problemático. La implementación final
evita recorrer todas las combinaciones posibles de etiquetas, para concentrarse
en extender sólo secuencias de etiquetas para las cuales ya se sabe que las
probabilidades de transición y de "output" involucradas son distintas de 0.   

* Ejercicio 5: HMM POS Tagger
    Se implementó un HMM Pos Tagger, estimando sus parámetros mediante
el método de máxima verosimilitud. El modelo resultante se entrenó y evaluó,
para distintos valores de n. También se midió el tiempo de ejecución del
algoritmo de evaluación, con el comando time. Se obtuvieron los siguientes resultados:
    _ n = 1:
        Accuracy: 89.01%
        Accuracy over known words: 95.32%
        Accuracy over unknown words: 31.80%
        
        real    1m15.365s
        user    0m38.212s
        sys     0m0.712s

    _ n = 2:
        Accuracy: 92.61%
        Accuracy over known words: 97.44%
        Accuracy over unknown words: 48.81%
        
        real    1m1.489s
        user    0m39.600s
        sys     0m0.840s

   _ n = 3:
        Accuracy: 92.52%
        Accuracy over known words: 96.98%
        Accuracy over unknown words: 52.18%
        
        real    2m55.396s
        user    2m32.540s
        sys 0m1.228s

        
    _ n = 4:
        Accuracy: 92.42%
        Accuracy over known words: 96.69%
        Accuracy over unknown words: 53.76%
        
        real    14m32.488s
        user    12m53.820s
        sys 0m6.316s

        
    
* Ejercicio 6: Features para Etiquetado de Secuencias
    Se implementaron las features solicitadas. La labor no presentó mayores
desafíos. La descripción de las mismas resultó directa. 
    
* Ejercicio 7: Maximum Entropy Markov Models
    Se implementó un Maximum Entropy Markov Model (MEMM), empleando la librería
featureforge (para la construcción de la matriz de valores de los 
"features" sobre los historiales) y sklearn (de donde se extrajo el clasificador
LogisticRegression, y la clase Pipeline, para facilitar el proceso de
vectorización de los historiales junto con los "features", 
y entrenamiento y posterior uso del clasificador).
    El MEMM se entrenó y evaluó, para distintos valores de n. Nuevamente se
tomó el tiempo de evaluación, con el comando time. Se obtuvieron los siguientes
resultados:

    _ n = 1:
        Accuracy: 92.03%
        Accuracy over known words: 95.26%
        Accuracy over unknown words: 62.75%
        
        real    1m35.271s
        user    1m32.424s
        sys     0m0.736s
        
    _n = 2:
        Accuracy: 91.32%
        Accuracy over known words: 94.55%
        Accuracy over unknown words: 62.07%
        
        real    1m46.164s
        user    1m41.952s
        sys     0m0.616s
        
    _n = 3:
        Accuracy: 91.55%
        Accuracy over known words: 94.69%
        Accuracy over unknown words: 63.09%
        
        real    2m1.554s
        user    1m58.800s
        sys     0m1.588s


    _n = 4:
        Accuracy: 91.61%
        Accuracy over known words: 94.69%
        Accuracy over unknown words: 63.62%
        
        real	2m14.706s
        user	2m13.668s
        sys	    0m0.596s

    Las matrices de confusión, en cualquier caso, muestran una diagonal poblada
con valores altos, y pocos valores distintos de cero, dispersos fuera de la
diagonal.
    Se experimentó también con los clasificadores sklearn.svm.LinearSVC y
sklearn.naive_bayes.MultinomialNB. Los resultados para sklearn.svm.LinearSVC,
fueron los siguientes:
        _n = 1:
            Accuracy: 93.78%
            Accuracy over known words: 97.03%
            Accuracy over unknown words: 64.36%
        
            real    2m2.295s
            user    1m29.408s
            sys     0m1.072s
            
        _n = 2:
            Accuracy: 93.63%
            Accuracy over known words: 96.88%
            Accuracy over unknown words: 64.12%
            
            real    1m37.871s
            user    1m27.256s
            sys     0m1.476s
            
        _n = 3:
            Accuracy: 93.82%
            Accuracy over known words: 96.93%
            Accuracy over unknown words: 65.58%
            
            real    2m3.801s
            user    1m51.712s
            sys     0m1.120s
            
        _n = 4:
            Accuracy: 93.91%
            Accuracy over known words: 96.95%
            Accuracy over unknown words: 66.32%
            
            real    2m8.276s
            user    2m6.616s
            sys     0m0.732s

    Con respecto a MultinomialNB, el tiempo de entrenamiento fue razonable, pero
la duración de la evaluación fue demasiado extenso. No ha sido posible
registrar el resultado.